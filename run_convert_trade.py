import os
import glob
import subprocess
import json
from pathlib import Path

from convert_logger import logger, safe_log
from convert_api import _sync_time
from quote_counter import can_request_quote
from utils_dev3 import safe_float
from convert_filters import normalize_pair

EXPLORE_MODE = int(os.getenv("EXPLORE_MODE", "0"))
EXPLORE_PAPER = int(os.getenv("EXPLORE_PAPER", "1"))
EXPLORE_MAX = int(os.getenv("EXPLORE_MAX", "2"))
EXPLORE_MIN_EDGE = float(os.getenv("EXPLORE_MIN_EDGE", "0.001"))
EXPLORE_MIN_LOT_FACTOR = float(os.getenv("EXPLORE_MIN_LOT_FACTOR", "0.5"))
MIN_NOTIONAL = float(os.getenv("MIN_NOTIONAL", "0") or 0.0)

logger.info(
    safe_log(
        f"[dev3] Explore: MODE={EXPLORE_MODE} PAPER={EXPLORE_PAPER} MAX={EXPLORE_MAX} "
        f"MIN_EDGE={EXPLORE_MIN_EDGE} MIN_LOT_FACTOR={EXPLORE_MIN_LOT_FACTOR}"
    )
)

if not can_request_quote():
    logger.warning(safe_log("[dev3] ‚õî –õ—ñ–º—ñ—Ç –∑–∞–ø–∏—Ç—ñ–≤ –¥–æ Convert API –¥–æ—Å—è–≥–Ω—É—Ç–æ. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ü–∏–∫–ª."))
    exit(0)

CACHE_FILES = [
    "signals.txt",
    "last_message.txt",
]


def load_top_pairs(path: str = "top_tokens.json") -> list[dict]:
    import logging
    log = logging.getLogger(__name__)
    p = Path(path)
    if not p.exists():
        p = Path("logs") / "top_tokens.json"
    try:
        data = json.loads(p.read_text())
    except Exception as e:
        log.error("failed to read top pairs: %s", e)
        return []
    out: list[dict] = []
    for raw in data:
        r, reason = normalize_pair(raw or {})
        if reason:
            log.info("skip pair (%s): %s", reason, raw)
            continue
        out.append(r)
    return out


def cleanup() -> None:
    for path in CACHE_FILES:
        if os.path.exists(path):
            try:
                os.remove(path)
            except OSError:
                pass
    for temp in glob.glob(os.path.join("logs", "temp_*.json")):
        try:
            os.remove(temp)
        except OSError:
            pass
    # [dev3] –ù–µ –≤–∏–¥–∞–ª—è—î–º–æ top_tokens.json –Ω–∞ –ø–æ—á–∞—Ç–∫—É —Ç—Ä–µ–π–¥-—Ü–∏–∫–ª—É, –±–æ —Ñ–∞–π–ª
    # —Å—Ç–≤–æ—Ä—é—î—Ç—å—Å—è –≤ GPT-–µ—Ç–∞–ø—ñ –∑–∞ –≥–æ–¥–∏–Ω—É –¥–æ —Ç—Ä–µ–π–¥—É
    for qfile in glob.glob(os.path.join("logs", "quote_*.json")):
        try:
            os.remove(qfile)
        except OSError:
            pass
    for log_path in glob.glob(os.path.join("logs", "*.log")):
        try:
            if os.path.getsize(log_path) > 5 * 1024 * 1024:
                os.remove(log_path)
        except OSError:
            pass


def main() -> None:
    cleanup()
    _sync_time()
    logger.info(safe_log("[dev3] üîÑ –ó–∞–ø—É—Å–∫ convert —Ç—Ä–µ–π–¥–∏–Ω–≥—É"))
    try:
        from convert_cycle import process_top_pairs

        logger.info(safe_log("[dev3] üìÑ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ —Ñ–∞–π–ª—É logs/top_tokens.json..."))
        path = Path("logs") / "top_tokens.json"
        if not path.exists():
            path = Path("top_tokens.json")
        if not path.exists():
            logger.warning(safe_log("[dev3] ‚õîÔ∏è –§–∞–π–ª logs/top_tokens.json –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –ó–∞–≤–µ—Ä—à—É—î–º–æ —Ü–∏–∫–ª."))
            return

        top_tokens = load_top_pairs(str(path))

        if not top_tokens:
            logger.warning(safe_log("[dev3] ‚õîÔ∏è –§–∞–π–ª top_tokens.json –ø–æ—Ä–æ–∂–Ω—ñ–π. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ç—Ä–µ–π–¥."))
            return

        logger.info(safe_log(f"[dev3] ‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(top_tokens)} –ø–∞—Ä –∑ {path.name}"))
        all_zero = all(
            safe_float(item.get("gpt", {}).get("score", item.get("score", 0))) == 0
            and safe_float(item.get("expected_profit", 0)) == 0
            for item in top_tokens
        )
        if all_zero:
            logger.warning(safe_log("[dev3] ‚ö†Ô∏è top_tokens all zeros ‚Äî GPT gating disabled"))
        config = {
            "mode": EXPLORE_MODE,
            "paper": EXPLORE_PAPER,
            "max": EXPLORE_MAX,
            "min_edge": EXPLORE_MIN_EDGE,
            "min_lot_factor": EXPLORE_MIN_LOT_FACTOR,
            "model_only": all_zero,
        }
        process_top_pairs(top_tokens, config)
    except Exception as e:
        logger.error(safe_log(f"[dev3] ‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ top_tokens.json: {e}"))
        return
    cleanup()
    logger.info(safe_log("[dev3] ‚úÖ –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à–µ–Ω–æ"))

    # üß† –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
    logger.info(safe_log("[dev3] üìö –ü–æ—á–∏–Ω–∞—î–º–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ..."))
    try:
        subprocess.run(["python3", "train_convert_model.py", "--force-train"], check=True)
    except Exception as exc:
        logger.error(safe_log(f"[dev3] ‚ùå –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å –∑ –ø–æ–º–∏–ª–∫–æ—é: {exc}"))
        return
    logger.info(safe_log("[dev3] ‚úÖ –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ"))

    predictions_path = os.path.join("logs", "predictions.json")
    if os.path.exists(predictions_path):
        try:
            os.remove(predictions_path)
        except OSError:
            pass


if __name__ == "__main__":
    main()
